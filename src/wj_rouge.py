import os
import json
import pandas as pd
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rouge_score import rouge_scorer

os.environ["CUDA_VISIBLE_DEVICES"] = "3" 
device = "cuda" if torch.cuda.is_available() else "cpu"

# ========== æ¨ç†å‡½æ•°ï¼ˆé€šç”¨ï¼‰==========
def predict(messages, model, tokenizer, max_new_tokens=2048):
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response.strip()

# ========== ROUGEè¯„ä¼°å‡½æ•° ==========
def calculate_rouge_scores(predictions, references, rouge_types=['rouge1', 'rouge2', 'rougeL']):
    """
    è®¡ç®—ROUGEåˆ†æ•°
    
    Args:
        predictions: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        references: å‚è€ƒæ–‡æœ¬ï¼ˆground truthï¼‰åˆ—è¡¨  
        rouge_types: ROUGEç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤åŒ…æ‹¬['rouge1', 'rouge2', 'rougeL']
    
    Returns:
        dict: åŒ…å«å„ç§ROUGEåˆ†æ•°çš„å­—å…¸
    """
    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)
    
    scores = {rouge_type: {'precision': [], 'recall': [], 'fmeasure': []} for rouge_type in rouge_types}
    
    for pred, ref in zip(predictions, references):
        # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
        pred = pred.strip() if pred.strip() else "empty"
        ref = ref.strip() if ref.strip() else "empty"
        
        score = scorer.score(ref, pred)
        
        for rouge_type in rouge_types:
            scores[rouge_type]['precision'].append(score[rouge_type].precision)
            scores[rouge_type]['recall'].append(score[rouge_type].recall)
            scores[rouge_type]['fmeasure'].append(score[rouge_type].fmeasure)
    
    # è®¡ç®—å¹³å‡å€¼
    avg_scores = {}
    for rouge_type in rouge_types:
        avg_scores[rouge_type] = {
            'precision': sum(scores[rouge_type]['precision']) / len(scores[rouge_type]['precision']),
            'recall': sum(scores[rouge_type]['recall']) / len(scores[rouge_type]['recall']),
            'fmeasure': sum(scores[rouge_type]['fmeasure']) / len(scores[rouge_type]['fmeasure'])
        }
    
    return avg_scores

# ========== æ•°æ®è½¬æ¢å‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰==========
def dataset_jsonl_transfer(src_path, dst_path):
    """æ¨¡æ‹Ÿæ ¼å¼è½¬æ¢ï¼šå°†åŸå§‹ test.jsonl è½¬ä¸º {instruction, input, output} æ ¼å¼"""
    try:
        with open(src_path, "r", encoding="utf-8") as f_in, open(dst_path, "w", encoding="utf-8") as f_out:
            for line in f_in:
                data = json.loads(line.strip())
                # å‡è®¾åŸå§‹æ ¼å¼æ˜¯ {'question': ..., 'answer': ..., 'cot': ...}
                # è¿™é‡ŒæŒ‰ä½ çš„è®­ç»ƒæ ¼å¼ç»Ÿä¸€ä¸º instruction + input + output
                item = {
                    "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                    "input": data.get("question", ""),
                    "output": data.get("answer", "")
                }
                f_out.write(json.dumps(item, ensure_ascii=False) + "\n")
        return True
    except Exception as e:
        print(f"è½¬æ¢å¤±è´¥: {e}")
        return False

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    # --- é…ç½®è·¯å¾„ ---
    test_dataset_path = "./test.jsonl"
    test_format_path = "./test_format.jsonl"

    # --- åŠ è½½å¾…è¯„ä¼°æ¨¡å‹---
    print("æ­£åœ¨åŠ è½½å¾…è¯„ä¼°æ¨¡å‹...")
    base_model_name_or_path = "Qwen/Qwen3-1.7B"
    tokenizer_target = AutoTokenizer.from_pretrained(base_model_name_or_path, use_fast=False, trust_remote_code=True)
    model_target = AutoModelForCausalLM.from_pretrained(
        base_model_name_or_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    lora_checkpoint = "./output/Qwen3-1.7B/checkpoint-1084"
    model_target = PeftModel.from_pretrained(model_target, lora_checkpoint)
    model_target = model_target.merge_and_unload()

    # --- åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰---
    print("æ­£åœ¨åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆQwen3-1.7Bï¼‰...")
    model_base = AutoModelForCausalLM.from_pretrained(
        base_model_name_or_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # --- å‡†å¤‡æµ‹è¯•æ•°æ® ---
    if os.path.exists(test_dataset_path):
        print("æ­£åœ¨è½¬æ¢æµ‹è¯•æ•°æ®é›†æ ¼å¼...")
        if not os.path.exists(test_format_path):
            if not dataset_jsonl_transfer(test_dataset_path, test_format_path):
                print("âš ï¸ è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
                test_texts, ground_truths = None, None
            else:
                print(f"âœ… è½¬æ¢æˆåŠŸï¼Œä¿å­˜è‡³ {test_format_path}")
        else:
            print(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„æ ¼å¼åŒ–æ•°æ®: {test_format_path}")

        if os.path.exists(test_format_path):
            test_df = pd.read_json(test_format_path, lines=True)
            test_texts = [
                {"instruction": row["instruction"], "input": row["input"]}
                for _, row in test_df.iterrows()
            ]
            ground_truths = test_df["output"].tolist()
        else:
            test_texts = None
    else:
        test_texts = None

    # --- Fallback to default test data ---
    if test_texts is None or len(test_texts) == 0:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
        test_texts = [
            {
                'instruction': "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                'input': "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘è¢«è¯Šæ–­ä¸ºç³–å°¿ç—…ï¼Œå¬è¯´ç¢³æ°´åŒ–åˆç‰©çš„é€‰æ‹©å¾ˆé‡è¦ï¼Œæˆ‘åº”è¯¥é€‰æ‹©ä»€ä¹ˆæ ·çš„ç¢³æ°´åŒ–åˆç‰©å‘¢ï¼Ÿ"
            },
            {
                'instruction': "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                'input': "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘èƒƒéƒ¨ä¸é€‚ï¼Œå¬è¯´æœ‰å‡ ç§æŠ—æºƒç–¡è¯ç‰©å¯ä»¥æ²»ç–—ï¼Œæ‚¨èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹è¿™äº›è¯ç‰©çš„åˆ†ç±»ã€ä½œç”¨æœºåˆ¶ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•å½±å“èƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡çš„å—ï¼Ÿ"
            }
        ]
        ground_truths = [
            "ç³–å°¿ç—…æ‚£è€…åº”é€‰æ‹©ä½å‡ç³–æŒ‡æ•°ï¼ˆGIï¼‰çš„ç¢³æ°´åŒ–åˆç‰©ï¼Œå¦‚å…¨è°·ç‰©ã€è±†ç±»ã€è”¬èœç­‰ï¼Œé¿å…ç²¾åˆ¶ç³–å’Œç™½é¢åŒ…ç­‰é«˜GIé£Ÿç‰©ï¼Œä»¥å¸®åŠ©æ§åˆ¶è¡€ç³–æ°´å¹³ã€‚",
            "æŠ—æºƒç–¡è¯ç‰©ä¸»è¦åŒ…æ‹¬è´¨å­æ³µæŠ‘åˆ¶å‰‚ï¼ˆå¦‚å¥¥ç¾æ‹‰å”‘ï¼‰ã€H2å—ä½“æ‹®æŠ—å‰‚ï¼ˆå¦‚é›·å°¼æ›¿ä¸ï¼‰å’Œèƒƒé»è†œä¿æŠ¤å‰‚ï¼ˆå¦‚ç¡«ç³–é“ï¼‰ã€‚å®ƒä»¬é€šè¿‡ä¸åŒæœºåˆ¶å‡å°‘èƒƒé…¸åˆ†æ³Œæˆ–ä¿æŠ¤èƒƒé»è†œï¼Œç»´æŒèƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡ã€‚"
        ]


    #test_texts = test_texts[:1]  # æµ‹è¯•


    print(f"âœ… å…±åŠ è½½ {len(test_texts)} æ¡æµ‹è¯•æ ·æœ¬")



    # --- ç”Ÿæˆæ¨¡å‹å“åº” ---
    print("æ­£åœ¨ç”Ÿæˆæ¨¡å‹å“åº”...")
    predictions_tuned = []
    predictions_base = []

    for i, (item, gt) in enumerate(tqdm(zip(test_texts, ground_truths), total=len(test_texts), desc="ç”Ÿæˆå“åº”")):
        try:
            messages = [
                {"role": "system", "content": item["instruction"]},
                {"role": "user", "content": item["input"]}
            ]
            # ç”Ÿæˆå¾®è°ƒæ¨¡å‹çš„å›ç­”
            cot_response_tuned = predict(messages, model_target, tokenizer_target)
            predictions_tuned.append(cot_response_tuned)
            
            # ç”ŸæˆåŸå§‹æ¨¡å‹çš„å›ç­”
            cot_response_base = predict(messages, model_base, tokenizer_target)
            predictions_base.append(cot_response_base)

        except Exception as e:
            print(f"\nâŒ ç¬¬ {i} æ¡æ ·æœ¬å¤„ç†å¤±è´¥: {e}")
            predictions_tuned.append("")
            predictions_base.append("")

    # --- è®¡ç®—ROUGEåˆ†æ•° ---
    print("æ­£åœ¨è®¡ç®—ROUGEåˆ†æ•°...")
    rouge_types = ['rouge1', 'rouge2', 'rougeL']
    
    # è®¡ç®—å¾®è°ƒæ¨¡å‹çš„ROUGEåˆ†æ•°
    rouge_scores_tuned = calculate_rouge_scores(predictions_tuned, ground_truths, rouge_types)
    
    # è®¡ç®—åŸå§‹æ¨¡å‹çš„ROUGEåˆ†æ•°
    rouge_scores_base = calculate_rouge_scores(predictions_base, ground_truths, rouge_types)

    # --- è¾“å‡ºæœ€ç»ˆç»“æœ ---
    print("\n" + "="*80)
    print("ğŸ“Š ROUGE è¯„ä¼°ç»“æœ:")
    print("="*80)
    
    print("-" * 80)
    print(f"æ¨¡å‹ç±»å‹        | ROUGEç±»å‹ | Precision | Recall  | F1      ")
    print("-" * 80)
    for rouge_type in rouge_types:
        # å¾®è°ƒæ¨¡å‹
        print(f"å¾®è°ƒæ¨¡å‹ (Tuned) | {rouge_type:<8} | {rouge_scores_tuned[rouge_type]['precision']:.4f}  | {rouge_scores_tuned[rouge_type]['recall']:.4f}  | {rouge_scores_tuned[rouge_type]['fmeasure']:.4f}")
        # åŸå§‹æ¨¡å‹
        print(f"åŸå§‹æ¨¡å‹ (Base)  | {rouge_type:<8} | {rouge_scores_base[rouge_type]['precision']:.4f}  | {rouge_scores_base[rouge_type]['recall']:.4f}  | {rouge_scores_base[rouge_type]['fmeasure']:.4f}")
        print("-" * 80)
    
    print(f"æ ·æœ¬æ€»æ•°: {len(test_texts)}")
    print("="*80)

    # --- è¾“å‡ºæ€§èƒ½æå‡ ---
    print("\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ (Tuned - Base):")
    print("-" * 50)
    for rouge_type in rouge_types:
        improvement_precision = rouge_scores_tuned[rouge_type]['precision'] - rouge_scores_base[rouge_type]['precision']
        improvement_recall = rouge_scores_tuned[rouge_type]['recall'] - rouge_scores_base[rouge_type]['recall']
        improvement_f1 = rouge_scores_tuned[rouge_type]['fmeasure'] - rouge_scores_base[rouge_type]['fmeasure']
        print(f"  {rouge_type} - Precision: {improvement_precision:+.4f}, Recall: {improvement_recall:+.4f}, F1: {improvement_f1:+.4f}")
    
    print("="*80)
    
    # --- å¯é€‰ï¼šä¿å­˜è¯¦ç»†ç»“æœ ---
    detailed_results = {
        "rouge_scores": {
            "tuned": {k: v for k, v in rouge_scores_tuned.items()},
            "base": {k: v for k, v in rouge_scores_base.items()}
        },
        "individual_results": [
            {
                "index": i,
                "question": test_texts[i]["input"],
                "ground_truth": ground_truths[i],
                "prediction_tuned": predictions_tuned[i],
                "prediction_base": predictions_base[i]
            }
            for i in range(len(test_texts))
        ]
    }
    
    with open("rouge_evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    
    print("âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ rouge_evaluation_results.json")