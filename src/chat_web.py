# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from peft import PeftModel
# import gradio as gr
#
# # -----------------------------
# # é¢„æµ‹å‡½æ•°ï¼ˆä¸ä½ é¡¹ç›®ä¿æŒä¸€è‡´ï¼‰
# # -----------------------------
# def predict(messages, model, tokenizer, device, max_new_tokens=512):
#     text = tokenizer.apply_chat_template(
#         messages,
#         tokenize=False,
#         add_generation_prompt=True
#     )
#
#     model_inputs = tokenizer([text], return_tensors="pt").to(device)
#
#     generated_ids = model.generate(
#         model_inputs.input_ids,
#         max_new_tokens=max_new_tokens,
#     )
#
#     # å»æ‰è¾“å…¥éƒ¨åˆ†ï¼Œä»…ä¿ç•™è¾“å‡º
#     generated_ids = [
#         output_ids[len(input_ids):]
#         for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
#     ]
#
#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
#     return response
#
#
# # -----------------------------
# # åˆ›å»º Gradio èŠå¤©ç•Œé¢
# # -----------------------------
# def create_chat_interface(model_path, lora_path=None):
#
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     print(f"åŠ è½½è®¾å¤‡: {device}")
#
#     # åŠ è½½ tokenizer
#     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)
#
#     # åŠ è½½åŸºç¡€æ¨¡å‹
#     model = AutoModelForCausalLM.from_pretrained(
#         model_path,
#         torch_dtype=torch.bfloat16,
#         device_map="auto"
#     )
#
#     # LoRAï¼ˆå¯é€‰ï¼‰
#     if lora_path:
#         print(f"åŠ è½½ LoRA æƒé‡: {lora_path}")
#         model = PeftModel.from_pretrained(model, lora_path)
#
#     messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚"}]
#
#     # -------------------
#     # Gradio å›è°ƒå‡½æ•°
#     # -------------------
#     def chat_fn(user_input, chat_history):
#         messages.append({"role": "user", "content": user_input})
#
#         response = predict(messages, model, tokenizer, device)
#
#         messages.append({"role": "assistant", "content": response})
#         chat_history.append((user_input, response))
#
#         return chat_history, ""
#
#     # -------------------
#     # æ„å»ºç•Œé¢
#     # -------------------
#     with gr.Blocks(title="åŒ»å­¦å¯¹è¯æ¨¡å‹ Demo") as demo:
#         gr.Markdown(
#             """
#             # ğŸ©º åŒ»å­¦å¤§æ¨¡å‹ Demo
#             #### æ”¯æŒå¤šè½®å¯¹è¯ã€LoRA å¾®è°ƒã€åŒ»å­¦é—®ç­”å±•ç¤º
#             """
#         )
#
#         chatbot = gr.Chatbot(
#             height=450,
#             label="èŠå¤©çª—å£"
#         )
#
#         with gr.Row():
#             user_input = gr.Textbox(
#                 placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦",
#                 scale=5
#             )
#             submit_btn = gr.Button("å‘é€", scale=1)
#
#         submit_btn.click(
#             fn=chat_fn,
#             inputs=[user_input, chatbot],
#             outputs=[chatbot, user_input]
#         )
#
#         user_input.submit(
#             fn=chat_fn,
#             inputs=[user_input, chatbot],
#             outputs=[chatbot, user_input]
#         )
#
#     return demo
#
#
# # -----------------------------
# # ä¸»å…¥å£
# # -----------------------------
# if __name__ == "__main__":
#     import argparse
#
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--model_path", type=str, required=True)
#     parser.add_argument("--lora_path", type=str, default=None)
#     parser.add_argument("--port", type=int, default=7860)
#
#     args = parser.parse_args()
#
#     demo = create_chat_interface(args.model_path, args.lora_path)
#     demo.launch(server_name="0.0.0.0", server_port=args.port, share=True)
#
#
#
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import gradio as gr
import re


# -----------------------------
# é¢„æµ‹ + åˆ†ç¦» think / answer
# -----------------------------
def predict(messages, model, tokenizer, device, max_new_tokens=1024):
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=max_new_tokens,
    )

    # å»æ‰è¾“å…¥ prompt
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    full_output = tokenizer.batch_decode(
        generated_ids,
        skip_special_tokens=True
    )[0]

    # -----------------------------
    # åˆ†ç¦» think ä¸ answer
    # -----------------------------
    think_pattern = r"<think>(.*?)</think>"
    think_match = re.search(think_pattern, full_output, re.S)

    if think_match:
        think = think_match.group(1).strip()
        # æœ€ç»ˆè¾“å‡ºå»æ‰ think å—
        answer = re.sub(think_pattern, "", full_output, flags=re.S).strip()
    else:
        think = ""
        answer = full_output.strip()

    return think, answer


# -----------------------------
# åˆ›å»º Gradio èŠå¤©ç•Œé¢
# -----------------------------
def create_chat_interface(model_path, lora_path=None):

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"åŠ è½½è®¾å¤‡: {device}")

    # ä¿®æ”¹è¿™ä¸€è¡Œ
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True, local_files_only=True)

    # ä¿®æ”¹è¿™ä¸€è¡Œ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        local_files_only=True
    )
    

    if lora_path:
        print(f"åŠ è½½ LoRA æƒé‡: {lora_path}")
        model = PeftModel.from_pretrained(model, lora_path)

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒï¼ˆthinkï¼‰çš„å›ç­”ã€‚"}
    ]

    # -------------------
    # Gradio å›è°ƒ
    # -------------------
    def chat_fn(user_input, chat_history, think_box):
        messages.append({"role": "user", "content": user_input})

        think, answer = predict(messages, model, tokenizer, device)

        messages.append({"role": "assistant", "content": answer})

        chat_history.append((user_input, answer))

        return chat_history, "", think

    # -------------------
    # æ„å»ºç•Œé¢
    # -------------------
    with gr.Blocks(title="åŒ»å­¦å¤§æ¨¡å‹ Qwen3-Medical-SFT") as demo:
        gr.Markdown(
            """
            # ğŸ©º åŒ»å­¦å¤§æ¨¡å‹ Qwen3-Medical-SFT
            ### âœ” æ”¯æŒå¤šè½®å¯¹è¯  
            ### âœ” LoRA å¾®è°ƒ  
            """
        )

        with gr.Row():
            chatbot = gr.Chatbot(
                height=450,
                label="æ¨¡å‹å›ç­”ï¼ˆAnswerï¼‰"
            )
            think_box = gr.Textbox(
                label="æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆThinkï¼‰",
                placeholder="æ¨¡å‹çš„ <think> æ€è€ƒè¿‡ç¨‹å°†åœ¨è¿™é‡Œæ˜¾ç¤ºâ€¦",
                lines=20
            )

        with gr.Row():
            user_input = gr.Textbox(
                placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦",
                scale=5
            )
            submit_btn = gr.Button("å‘é€", scale=1)

        submit_btn.click(
            fn=chat_fn,
            inputs=[user_input, chatbot, think_box],
            outputs=[chatbot, user_input, think_box]
        )

        user_input.submit(
            fn=chat_fn,
            inputs=[user_input, chatbot, think_box],
            outputs=[chatbot, user_input, think_box]
        )

    return demo


# -----------------------------
# ä¸»å…¥å£
# -----------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--lora_path", type=str, default=None)
    parser.add_argument("--port", type=int, default=7860)

    args = parser.parse_args()

    demo = create_chat_interface(args.model_path, args.lora_path)
    demo.launch(server_name="0.0.0.0", server_port=args.port, share=True)
