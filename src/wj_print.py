import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ========== æ¨ç†å‡½æ•°ï¼ˆé€šç”¨ï¼‰==========
def predict(messages, model, tokenizer, max_new_tokens=2048):
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response.strip()

# ========== ä¸»ç¨‹åºï¼šä»…è¿è¡Œ Base æ¨¡å‹ ==========
if __name__ == "__main__":
    # åŠ è½½ Base æ¨¡å‹ï¼ˆQwen3-1.7Bï¼‰
    base_model_name_or_path = "Qwen/Qwen3-1.7B"
    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path, use_fast=False, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name_or_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # æµ‹è¯•è¾“å…¥
    instruction = "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚"
    # instruction = "åŒ»ç”Ÿï¼Œæˆ‘å¬è¯´å‡è¡€å—æœºåŒ–åä¼šå¯¹å‘¼å¸åŠŸèƒ½é€ æˆæŸå®³ï¼Œè¿™æ˜¯çœŸçš„å—ï¼Ÿå…·ä½“æ˜¯æ€ä¹ˆå½±å“çš„å‘¢ï¼Ÿè€Œä¸”ï¼Œè¿™ç§å½±å“åœ¨ä¸åŒç±»å‹çš„è¡€èƒ¸ä¸­æœ‰ä»€ä¹ˆä¸åŒå—ï¼Ÿ"
    input_value = "åŒ»ç”Ÿï¼Œæˆ‘å¬è¯´å‡è¡€å—æœºåŒ–åä¼šå¯¹å‘¼å¸åŠŸèƒ½é€ æˆæŸå®³ï¼Œè¿™æ˜¯çœŸçš„å—ï¼Ÿå…·ä½“æ˜¯æ€ä¹ˆå½±å“çš„å‘¢ï¼Ÿè€Œä¸”ï¼Œè¿™ç§å½±å“åœ¨ä¸åŒç±»å‹çš„è¡€èƒ¸ä¸­æœ‰ä»€ä¹ˆä¸åŒå—ï¼Ÿ"

    messages = [
        {"role": "system", "content": instruction},
        {"role": "user", "content": input_value}
    ]

    # ä»…è¿è¡Œ Base æ¨¡å‹æ¨ç†
    base_response = predict(messages, model, tokenizer)
    print("ğŸŸ¨ åŸå§‹åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å›ç­”ï¼š")
    print(base_response)