import os
import json
import pandas as pd
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import snapshot_download
os.environ["CUDA_VISIBLE_DEVICES"] = "2" 
device = "cuda" if torch.cuda.is_available() else "cpu"
# ========== æ¨ç†å‡½æ•°ï¼ˆé€šç”¨ï¼‰==========
def predict(messages, model, tokenizer, max_new_tokens=2048):
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response.strip()

# ========== æ€ç»´é“¾åˆ†æ­¥ ==========
import re
from typing import List

def step_partition(cot_text: str, eval_model, eval_tokenizer, max_retries=2) -> List[str]:
    prompt = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘åˆ†æä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„æ¨ç†è¿‡ç¨‹æ‹†è§£ä¸ºè‹¥å¹²ä¸ªæ¸…æ™°ã€ç‹¬ç«‹ã€è¯­ä¹‰å®Œæ•´çš„æ¨ç†æ­¥éª¤ã€‚æ¯ä¸ªæ­¥éª¤åº”è¡¨è¾¾ä¸€ä¸ªå®Œæ•´çš„æ€æƒ³æˆ–äº‹å®ã€‚è¯·ä»¥ JSON åˆ—è¡¨æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"},
        {"role": "user", "content": f"æ¨ç†è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\n{cot_text}\n\nè¯·æ‹†è§£ä¸ºæ­¥éª¤ï¼ˆJSON åˆ—è¡¨æ ¼å¼ï¼‰ï¼š"}
    ]

    raw_output = predict(prompt, eval_model, eval_tokenizer, max_new_tokens=1024)

    for _ in range(max_retries):
        try:
            json_match = re.search(r"\[\s*\".*?\"\s*\]", raw_output, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                steps = json.loads(json_str)
                if isinstance(steps, list) and all(isinstance(s, str) for s in steps):
                    steps = [s.strip() for s in steps if s.strip()]
                    if steps:
                        return steps
            raw_output = predict(
                prompt + [{"role": "assistant", "content": raw_output}, {"role": "user", "content": "è¯·ä¸¥æ ¼æŒ‰ JSON åˆ—è¡¨æ ¼å¼è¾“å‡ºï¼Œä¾‹å¦‚ï¼š[\"æ­¥éª¤1\", \"æ­¥éª¤2\"]"}],
                eval_model,
                eval_tokenizer,
                max_new_tokens=512
            )
        except (json.JSONDecodeError, TypeError, KeyError):
            continue

    print("âš ï¸ LLM åˆ†æ­¥å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™åˆ†æ­¥")
    fallback_steps = re.split(r'\n\s*(?:\d+\.|-|\*|â€¢)\s*', cot_text)
    fallback_steps = [s.strip() for s in fallback_steps if s.strip()]
    if len(fallback_steps) <= 1:
        fallback_steps = [s.strip() for s in cot_text.split('ã€‚') if s.strip()]
    return fallback_steps if fallback_steps else [cot_text.strip()]

# ========== è¯„ä¼°å•ä¸ªæ ·æœ¬ ==========
def evaluate_cot_quality(question, model_response_cot, ground_truth, eval_model, eval_tokenizer):
    steps = step_partition(model_response_cot, eval_model, eval_tokenizer)
    if not steps:
        return {"recall": 0.0, "precision": 0.0, "f1": 0.0, "num_steps": 0}

    recall_hits = 0
    precision_hits = 0

    for step in steps:
        # Recall: æ˜¯å¦è¢« ground truth æ”¯æŒ
        recall_prompt = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ¨ç†æ­¥éª¤çš„å†…å®¹æ˜¯å¦å¯ä»¥ä»çœŸå®ç­”æ¡ˆä¸­æ¨æ–­å‡ºï¼ˆå³çœŸå®ç­”æ¡ˆæ˜¯å¦æ”¯æŒè¯¥æ­¥éª¤ï¼‰ã€‚åªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚"},
            {"role": "user", "content": f"çœŸå®ç­”æ¡ˆï¼š{ground_truth}\n\næ¨ç†æ­¥éª¤ï¼š{step}"}
        ]
        recall_ans = predict(recall_prompt, eval_model, eval_tokenizer, max_new_tokens=10).strip()
        is_supported = "æ˜¯" in recall_ans

        # Precision: æ­¥éª¤æœ¬èº«æ˜¯å¦æ­£ç¡®
        prec_prompt = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ¨ç†æ­¥éª¤åœ¨äº‹å®å’Œé€»è¾‘ä¸Šæ˜¯å¦æ­£ç¡®ã€‚åªå›ç­”â€œæ­£ç¡®â€æˆ–â€œé”™è¯¯â€ã€‚"},
            {"role": "user", "content": f"é—®é¢˜ï¼š{question}\n\næ¨ç†æ­¥éª¤ï¼š{step}"}
        ]
        prec_ans = predict(prec_prompt, eval_model, eval_tokenizer, max_new_tokens=10).strip()
        is_correct = "æ­£ç¡®" in prec_ans

        if is_supported:
            recall_hits += 1
        if is_correct:
            precision_hits += 1

    total = len(steps)
    recall = recall_hits / total if total > 0 else 0
    precision = precision_hits / total if total > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        "recall": round(recall, 4),
        "precision": round(precision, 4),
        "f1": round(f1, 4),
        "num_steps": total
    }

# ========== æ•°æ®è½¬æ¢å‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰==========
def dataset_jsonl_transfer(src_path, dst_path):
    """æ¨¡æ‹Ÿæ ¼å¼è½¬æ¢ï¼šå°†åŸå§‹ test.jsonl è½¬ä¸º {instruction, input, output} æ ¼å¼"""
    try:
        with open(src_path, "r", encoding="utf-8") as f_in, open(dst_path, "w", encoding="utf-8") as f_out:
            for line in f_in:
                data = json.loads(line.strip())
                # å‡è®¾åŸå§‹æ ¼å¼æ˜¯ {'question': ..., 'answer': ..., 'cot': ...}
                # è¿™é‡ŒæŒ‰ä½ çš„è®­ç»ƒæ ¼å¼ç»Ÿä¸€ä¸º instruction + input + output
                item = {
                    "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                    "input": data.get("question", ""),
                    "output": data.get("answer", "")
                }
                f_out.write(json.dumps(item, ensure_ascii=False) + "\n")
        return True
    except Exception as e:
        print(f"è½¬æ¢å¤±è´¥: {e}")
        return False

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    # --- é…ç½®è·¯å¾„ ---
    test_dataset_path = "./test.jsonl"
    test_format_path = "./test_format.jsonl"

    # --- åŠ è½½å¾…è¯„ä¼°æ¨¡å‹ï¼ˆLoRA å¾®è°ƒï¼‰---
    print("æ­£åœ¨åŠ è½½å¾…è¯„ä¼°æ¨¡å‹ï¼ˆLoRA å¾®è°ƒç‰ˆï¼‰...")
    base_model_name_or_path = "Qwen/Qwen3-1.7B"
    tokenizer_target = AutoTokenizer.from_pretrained(base_model_name_or_path, use_fast=False, trust_remote_code=True)
    model_target = AutoModelForCausalLM.from_pretrained(
        base_model_name_or_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    lora_checkpoint = "./output/Qwen3-1.7B-lora/checkpoint-1084"
    model_target = PeftModel.from_pretrained(model_target, lora_checkpoint)
    model_target = model_target.merge_and_unload()

    # --- åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰---
    print("æ­£åœ¨åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆQwen3-1.7Bï¼‰...")
    model_base = AutoModelForCausalLM.from_pretrained(
        base_model_name_or_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # --- åŠ è½½è¯„ä¼°æ¨¡å‹ï¼ˆQwen2-7B-Instructï¼‰---
    # --- åŠ è½½è¯„ä¼°æ¨¡å‹ï¼ˆQwen2-7B-Instructï¼‰---
    print("æ­£åœ¨åŠ è½½è¯„ä¼°æ¨¡å‹...")
    #eval_model_path = "Qwen/Qwen2-7B-Instruct"  # ç›´æ¥ä½¿ç”¨æ¨¡å‹ID
    
    eval_model_path = "./models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c"
    try:
        tokenizer_eval = AutoTokenizer.from_pretrained(eval_model_path, use_fast=False, trust_remote_code=True)
        model_eval = AutoModelForCausalLM.from_pretrained(
            eval_model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    except Exception as e:
        print("âŒ è¯„ä¼°æ¨¡å‹ Qwen2-7B-Instruct åŠ è½½å¤±è´¥")
        print("é”™è¯¯:", e)
        exit(1)

    


    # --- å‡†å¤‡æµ‹è¯•æ•°æ® ---
    if os.path.exists(test_dataset_path):
        print("æ­£åœ¨è½¬æ¢æµ‹è¯•æ•°æ®é›†æ ¼å¼...")
        if not os.path.exists(test_format_path):
            if not dataset_jsonl_transfer(test_dataset_path, test_format_path):
                print("âš ï¸ è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
                test_texts, ground_truths = None, None
            else:
                print(f"âœ… è½¬æ¢æˆåŠŸï¼Œä¿å­˜è‡³ {test_format_path}")
        else:
            print(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„æ ¼å¼åŒ–æ•°æ®: {test_format_path}")

        if os.path.exists(test_format_path):
            test_df = pd.read_json(test_format_path, lines=True)
            test_texts = [
                {"instruction": row["instruction"], "input": row["input"]}
                for _, row in test_df.iterrows()
            ]
            ground_truths = test_df["output"].tolist()
        else:
            test_texts = None
    else:
        test_texts = None

    # --- Fallback to default test data ---
    if test_texts is None or len(test_texts) == 0:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
        test_texts = [
            {
                'instruction': "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                'input': "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘è¢«è¯Šæ–­ä¸ºç³–å°¿ç—…ï¼Œå¬è¯´ç¢³æ°´åŒ–åˆç‰©çš„é€‰æ‹©å¾ˆé‡è¦ï¼Œæˆ‘åº”è¯¥é€‰æ‹©ä»€ä¹ˆæ ·çš„ç¢³æ°´åŒ–åˆç‰©å‘¢ï¼Ÿ"
            },
            {
                'instruction': "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                'input': "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘èƒƒéƒ¨ä¸é€‚ï¼Œå¬è¯´æœ‰å‡ ç§æŠ—æºƒç–¡è¯ç‰©å¯ä»¥æ²»ç–—ï¼Œæ‚¨èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹è¿™äº›è¯ç‰©çš„åˆ†ç±»ã€ä½œç”¨æœºåˆ¶ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•å½±å“èƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡çš„å—ï¼Ÿ"
            }
        ]
        ground_truths = [
            "ç³–å°¿ç—…æ‚£è€…åº”é€‰æ‹©ä½å‡ç³–æŒ‡æ•°ï¼ˆGIï¼‰çš„ç¢³æ°´åŒ–åˆç‰©ï¼Œå¦‚å…¨è°·ç‰©ã€è±†ç±»ã€è”¬èœç­‰ï¼Œé¿å…ç²¾åˆ¶ç³–å’Œç™½é¢åŒ…ç­‰é«˜GIé£Ÿç‰©ï¼Œä»¥å¸®åŠ©æ§åˆ¶è¡€ç³–æ°´å¹³ã€‚",
            "æŠ—æºƒç–¡è¯ç‰©ä¸»è¦åŒ…æ‹¬è´¨å­æ³µæŠ‘åˆ¶å‰‚ï¼ˆå¦‚å¥¥ç¾æ‹‰å”‘ï¼‰ã€H2å—ä½“æ‹®æŠ—å‰‚ï¼ˆå¦‚é›·å°¼æ›¿ä¸ï¼‰å’Œèƒƒé»è†œä¿æŠ¤å‰‚ï¼ˆå¦‚ç¡«ç³–é“ï¼‰ã€‚å®ƒä»¬é€šè¿‡ä¸åŒæœºåˆ¶å‡å°‘èƒƒé…¸åˆ†æ³Œæˆ–ä¿æŠ¤èƒƒé»è†œï¼Œç»´æŒèƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡ã€‚"
        ]

    #test_texts = test_texts[:10]
    #ground_truths = ground_truths[:10]

    print(f"âœ… å…±åŠ è½½ {len(test_texts)} æ¡æµ‹è¯•æ ·æœ¬")

    # --- å¼€å§‹æ‰¹é‡è¯„ä¼° ---
    all_results_tuned = []
    all_results_base = []
    for i, (item, gt) in enumerate(tqdm(zip(test_texts, ground_truths), total=len(test_texts), desc="è¯„ä¼°è¿›åº¦")):
        try:
            messages = [
                {"role": "system", "content": item["instruction"]},
                {"role": "user", "content": item["input"]}
            ]
            # ç”Ÿæˆå¾®è°ƒæ¨¡å‹çš„å›ç­”
            cot_response_tuned = predict(messages, model_target, tokenizer_target)
            # ç”ŸæˆåŸå§‹æ¨¡å‹çš„å›ç­”
            cot_response_base = predict(messages, model_base, tokenizer_target)  # æ³¨æ„ï¼štokenizer ç”¨åŒä¸€ä¸ª

            # è¯„ä¼°å¾®è°ƒæ¨¡å‹çš„å›ç­”
            result_tuned = evaluate_cot_quality(
                question=item["input"],
                model_response_cot=cot_response_tuned,
                ground_truth=gt,
                eval_model=model_eval,
                eval_tokenizer=tokenizer_eval
            )
            all_results_tuned.append(result_tuned)

            # è¯„ä¼°åŸå§‹æ¨¡å‹çš„å›ç­”
            result_base = evaluate_cot_quality(
                question=item["input"],
                model_response_cot=cot_response_base,
                ground_truth=gt,
                eval_model=model_eval,
                eval_tokenizer=tokenizer_eval
            )
            all_results_base.append(result_base)

        except Exception as e:
            print(f"\nâŒ ç¬¬ {i} æ¡æ ·æœ¬è¯„ä¼°å¤±è´¥: {e}")
            all_results_tuned.append({"recall": 0.0, "precision": 0.0, "f1": 0.0, "num_steps": 0})
            all_results_base.append({"recall": 0.0, "precision": 0.0, "f1": 0.0, "num_steps": 0})

    # --- è®¡ç®—å¹³å‡æŒ‡æ ‡ ---
    total = len(test_texts)

    # å¾®è°ƒæ¨¡å‹æŒ‡æ ‡
    avg_recall_tuned = sum(r["recall"] for r in all_results_tuned) / total if total > 0 else 0
    avg_precision_tuned = sum(r["precision"] for r in all_results_tuned) / total if total > 0 else 0
    avg_f1_tuned = sum(r["f1"] for r in all_results_tuned) / total if total > 0 else 0
    avg_steps_tuned = sum(r["num_steps"] for r in all_results_tuned) / total if total > 0 else 0

    # åŸå§‹æ¨¡å‹æŒ‡æ ‡
    avg_recall_base = sum(r["recall"] for r in all_results_base) / total if total > 0 else 0
    avg_precision_base = sum(r["precision"] for r in all_results_base) / total if total > 0 else 0
    avg_f1_base = sum(r["f1"] for r in all_results_base) / total if total > 0 else 0
    avg_steps_base = sum(r["num_steps"] for r in all_results_base) / total if total > 0 else 0

    # --- è¾“å‡ºæœ€ç»ˆç»“æœ ---
    print("\n" + "="*80)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœï¼ˆå¹³å‡å€¼ï¼‰:")
    print("-" * 80)
    print(f"æ¨¡å‹ç±»å‹        | Recall  | Precision | F1      | Avg Steps")
    print("-" * 80)
    print(f"å¾®è°ƒæ¨¡å‹ (Tuned) | {avg_recall_tuned:.4f}  | {avg_precision_tuned:.4f}     | {avg_f1_tuned:.4f}  | {avg_steps_tuned:.2f}")
    print(f"åŸå§‹æ¨¡å‹ (Base)  | {avg_recall_base:.4f}  | {avg_precision_base:.4f}     | {avg_f1_base:.4f}  | {avg_steps_base:.2f}")
    print("-" * 80)
    print(f"æ ·æœ¬æ€»æ•°: {total}")
    print("="*80)

    # --- å¯é€‰ï¼šè¾“å‡ºæ€§èƒ½æå‡ ---
    improvement_recall = avg_recall_tuned - avg_recall_base
    improvement_precision = avg_precision_tuned - avg_precision_base
    improvement_f1 = avg_f1_tuned - avg_f1_base
    print(f"ğŸ“ˆ æ€§èƒ½æå‡ (Tuned - Base):")
    print(f"  Recall:    {improvement_recall:+.4f}")
    print(f"  Precision: {improvement_precision:+.4f}")
    print(f"  F1:        {improvement_f1:+.4f}")
    print("="*80)
